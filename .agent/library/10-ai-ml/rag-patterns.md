# RAG - Retrieval Augmented Generation

> **v1.0.0** | **2026-01-09** | **LangChain, Vector DBs, LLMs**

---

## ğŸ”´ MUST

- [ ] **Smart Chunking** - Belgeyi mantÄ±klÄ± parÃ§alara bÃ¶l
- [ ] **Quality Embeddings** - Kaliteli embedding model kullan
- [ ] **Vector Database** - Embedding'leri verimli sakla
- [ ] **Relevance Ranking** - SonuÃ§larÄ± alaka gÃ¶re sÄ±rala

```typescript
// RAG pipeline
class RAGService {
  async query(question: string, topK: number = 5): Promise<string> {
    // 1. Soruyu embed et
    const queryEmbedding = await this.embed(question);

    // 2. Ä°lgili chunk'larÄ± getir
    const chunks = await this.vectorStore.search(queryEmbedding, topK);

    // 3. Context oluÅŸtur
    const context = chunks.map(c => c.text).join('\n\n');

    // 4. Context ile cevap Ã¼ret
    const prompt = `Context:\n${context}\n\nQuestion: ${question}`;
    return this.llm.generate(prompt);
  }

  // Smart chunking
  chunkDocument(text: string, maxSize: number = 500): string[] {
    return text.split(/\n\n+/)
      .reduce((acc: string[], para) => {
        if (acc.length === 0 || acc[acc.length - 1].length + para.length > maxSize) {
          acc.push(para);
        } else {
          acc[acc.length - 1] += '\n\n' + para;
        }
        return acc;
      }, []);
  }
}
```

---

## ğŸŸ¡ SHOULD

- [ ] **Hybrid Search** - Vector + keyword search
- [ ] **Reranking** - Ä°kinci katman reranking
- [ ] **Context Window** - Context length'i optimize et
- [ ] **Citations** - Kaynak gÃ¶ster

```typescript
// Hybrid search
async hybridSearch(query: string, alpha: number = 0.5) {
  const [vectorResults, keywordResults] = await Promise.all([
    this.vectorSearch(query),
    this.keywordSearch(query)
  ]);

  // SkorlarÄ± birleÅŸtir
  return this.mergeScores(vectorResults, keywordResults, alpha);
}

// Reranking
async rerank(query: string, results: Document[]): Promise<Document[]> {
  const reranked = await this.reranker.rerank(query, results);
  return reranked.slice(0, 10);
}
```

---

## â›” NEVER

- [ ] **Never Skip Chunking** - Chunking'siz RAG yok
- [ ] **Never Ignore Relevance** - Alaka skorunu Ã¶nemse
- [ ] **Never Overload Context** - Context window'u ÅŸiÅŸirme
- [ ] **Never Stale Data** - Eski embedding'leri temizle

```typescript
// âŒ YANLIÅ
const chunks = text.split('.'); // Naive chunking
const allDocs = await db.getAll(); // TÃ¼m veriyi Ã§ek

// âœ… DOÄRU
const chunks = chunkDocument(text, 500);
const relevant = await vectorStore.search(embedding, 5);
```

---

## ğŸ”— Referanslar

- [LangChain RAG](https://python.langchain.com/docs/use_cases/question_answering/)
- [Vector DB Guide](https://www.anthropic.com/index/vector-databases)
- [RAG Patterns](https://github.com/pinecone-io/pinecone-tutorials)
- [Chunking Strategies](https://www.llamaindex.ai/blog/chunking-strategies)
